---
title: "Assignment 4"
author: "Yue Zhang"
date: "`r Sys.Date()`"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
#This code chunk will tidy your knit PDF files, wrapping long code lines
#For it to work, the "formatR" package needs to be installed
#install.packages('formatR')
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60), tidy=TRUE)

```

```{r library, results= 'hide'}
setwd("/Users/yuezhang/Documents/Biostat/PH1976")
getwd()
library(ISLR2)
library(ggplot2)
library(tidyr)
library(dtplyr)
library(tidyverse)
library(ggcorrplot)
library(MASS)
library(boot)
library(class)
```

*3*\. 

**(a)**\. 
We divide the dataset into K equal-sized (or nearly equal) subsets (or folds). For each fold, one subset is held out as the test set, and we train on the remaining K-1 subsets and evaluated on the test fold. The process is repeated K times with a different fold being used as the test set each time\. 

**(b)**\. 
Validation-set approach: When using a validation set, we can only train on a small portion of the data as we must reserve the rest for validation. As a result it can overestimate the test error rate. The estimate of validation set error can be highly variable as it depends on which observations are used in the train and test dataset. However, the validation-set approach has less computational consumption since we only need to perform the model once. K-fold has less variance in result that validation set approach and provides a more reliable estimate of model performance compared to a simple train-val split. However, it increases computational complexity\. 

LOOCV: We can train on n-1 observations and test on the left out observation. As a result, LOOCV can have high variance and risk of overfitting as we catch all the noises. It is also costly in terms of processing time. This approach is more preferred for small datasets. K-fold approach is less consuming than LOOCV and lower variance\. 

*4*\. 

We could use bootstrap by resampling the training data many times. Then refit the model and make a prediction at the target X each time. Finally, we will compute the standard deviation of these bootstrap predictions across the samples\. 

*6*\. 

**(a)**\. 

```{r 6a, message = FALSE, warning = FALSE}
set.seed(123)

model_6a = glm(data = Default, default ~ income + balance, family = binomial)
summary(model_6a)
```
The estimated standard error for income is 4.985e-06, the estimated standard error for balance is 2.274e-04\. 

**(b)**\. 

```{r 6b, message = FALSE, warning = FALSE}
boot.fn = function(x, i){
  model = glm(data = x[i, ], default ~ income + balance, family = binomial)
  c(income = coef(model)["income"],
    balance = coef(model)["balance"])
}

boot.fn(Default, sample(seq_len(nrow(Default)), replace = TRUE))
```

**(c)**\. 

```{r 6c, message = FALSE, warning = FALSE}
boot(Default, boot.fn, R = 999)
```

**(d)**\. 
The standard errors obtained by bootstrapping are similar to what we've obtained from part(a)\. 

*8*\. 

**(a)**\. 

```{r 8a, message = FALSE, warning = FALSE}
set.seed(1)
x = rnorm(100)
y = x - 2 * x ^ 2 + rnorm(100)
```
${n} = 100$, ${p} = 1$, ${y} = -2{x}^2 + x + \epsilon$

**(b)**\. 

```{r 8b, message = FALSE, warning = FALSE}
plot(
  x,
  y,
  xlab = "X",
  ylab = "Y",
  pch = 19,
  col = "skyblue"
)
```
By looking at the plot we can see that y has a negative quadratic relationship with x.

**(c)**\. 

```{r 8c, message = FALSE, warning = FALSE}
set.seed(123)
df = data.frame(x, y)
sapply(1:4, function(i) cv.glm(data = df, glm(y ~ poly(x, i)))$delta[1])
```

**(d)**\. 

```{r 8d, message = FALSE, warning = FALSE}
set.seed(1215)
sapply(1:4, function(i) cv.glm(data = df, glm(y ~ poly(x, i)))$delta[1])
```
The results are the same as for each iteration, one data point is left out as the test set, and the model is trained on the remaining data for LOOCV\. 

**(e)**\. 

The model: ${y} = \beta_0 + \beta_1*{x} + \beta_2*x^2 + \epsilon$ has the smallest LOOCV. Because the data is in quadratic form and we are measuring the test error rate to evaluate performance\. 

**(f)**\. 

```{r 8f, message = FALSE, warning = FALSE}
for (i in 1:4)
  printCoefmat(coef(summary(glm(data = df, y ~ poly(
    x, i
  ))))
  )
```
From the summary we can see that only the coefficients in model 2 are significant as they all have p-values less than 0.05. Even we add more higher terms in later models, those coefficients are not significant. Therefore, the results agree with the conclusions drawn based on the cross-validation results.



